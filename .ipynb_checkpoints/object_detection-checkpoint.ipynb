{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7f4ca9-e65d-4b47-b1ab-424a55644c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\BineshMahato/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-12-24 Python-3.11.5 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\BineshMahato\\\\Desktop\\\\c.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBineshMahato\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mc.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Use your local image path here\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Read the image\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n\u001b[0;32m     14\u001b[0m img_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\ultralytics\\utils\\patches.py:26\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(filename, flags)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Read an image from a file.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        (np.ndarray): The read image.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mimdecode(np\u001b[38;5;241m.\u001b[39mfromfile(filename, np\u001b[38;5;241m.\u001b[39muint8), flags)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\BineshMahato\\\\Desktop\\\\c.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained YOLOv5 model (you can choose 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Load an image (replace 'your_image.jpg' with your test image path)\n",
    "img_path = r\"C:\\Users\\BineshMahato\\Desktop\\c.jpeg\"  # Use your local image path here\n",
    "\n",
    "# Read the image\n",
    "img = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Perform inference\n",
    "results = model(img_rgb)\n",
    "\n",
    "# Display the results inline in JupyterLab\n",
    "results.show()  # This will display the image with bounding boxes around pedestrians\n",
    "\n",
    "# You can also access the raw prediction data like labels and confidence\n",
    "labels = results.names  # Class names (e.g., 'person', 'car')\n",
    "predictions = results.pred[0]  # Detected predictions (bounding boxes, class ids, and confidence)\n",
    "\n",
    "# Filter predictions to only get 'person' (class 0 for 'person' in COCO dataset)\n",
    "person_predictions = predictions[predictions[:, 5] == 0]\n",
    "\n",
    "# Print predictions\n",
    "for *box, conf, cls in person_predictions:\n",
    "    print(f'Person detected with confidence {conf:.2f} at coordinates {box}')\n",
    "\n",
    "# Save the image with bounding boxes drawn on it\n",
    "# Render results on the image\n",
    "results.render()  # This will add bounding boxes to the image\n",
    "\n",
    "# Assuming results is a Detections object returned by the model\n",
    "results.render()  # Add bounding boxes to the image\n",
    "\n",
    "# Get the first image in the batch\n",
    "rendered_image = results.ims[0]\n",
    "\n",
    "# Save or display the image\n",
    "output_path = r\"C:\\Users\\BineshMahato\\Desktop\\output.jpg\"\n",
    "cv2.imwrite(output_path, rendered_image)  # Save the image with bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc8480-24b0-4614-b489-7fb9cc2bc774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e8a48-3c07-4369-a372-4e73cc02b93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
